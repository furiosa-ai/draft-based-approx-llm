import torch

from dataclasses import dataclass, field
from typing import Optional


@dataclass
class DraftApproxLLMConfig:
    """
    Configuration for Draft-based Approximate LLM.
    """

    max_capacity_prompt: int = field(
        default=1024,
        metadata={"help": "Maximum capacity of the KV cache/prompt."}
    )

    window_size: int = field(
        default=64,
        metadata={"help": "Window size (final tokens of input prompt)."}
    )

    pool_type: str = field(
        default="max",
        metadata={
            "help": "Type of pooling to use. Options: 'max', 'avg'."
        }
    )

    kernel_size: int = field(
        default=64,
        metadata={"help": "Pool kernel size."}
    )

    reduction_type: str = field(
        default="max",
        metadata={
            "help": "Type of reduction to use. Options: 'max', 'mean'."
        }
    )

    lookahead_tokens: Optional[int] = field(
        default=1,
        metadata={
            "help": "Number of lookahead tokens to consider."
        }
    )

    @staticmethod
    def from_dict(config_dict: dict):
        """
        Create a DraftApproxLLMConfig from a dictionary.
        """
        config_dict = {**config_dict}
        sparse_type = config_dict.pop("sparse_type")
        cls = {
            "speckv": SpecKVConfig,
            "specpc": SpecPCConfig,
        }[sparse_type]

        return cls(**config_dict)



@dataclass
class SpecKVConfig(DraftApproxLLMConfig):
    """
    Configuration for SpecKV.
    """

    lookahead_tokens: Optional[int] = field(
        default=None,
        metadata={
            "help": "Number of lookahead tokens to consider for SpecKV."
        }
    )

    prefill_window_size: int = field(
        default=2048,
        metadata={
            "help": "Number of window prefill tokens for SpecKV."
        }
    )

    prefill_vertical_size: int = field(
        default=2048,
        metadata={
            "help": "Number of vertical prefill tokens for SpecKV."
        }
    )

    @property
    def sparse_type(self):
        return "speckv"


@dataclass
class SpecPCConfig(DraftApproxLLMConfig):
    """
    Configuration for SpecPC.
    """

    lookahead_tokens: Optional[int] = field(
        default=1,
        metadata={
            "help": "Number of lookahead tokens to consider for SpecPC."
        }
    )

    neighbor_tokens: Optional[int] = field(
        default=None,
        metadata={
            "help": "Number of neighbors to consider for SpecPC. If None, it will be set to kernel_size."
        }
    )

    starting_layer_index: int = field(
        default=0,
        metadata={
            "help": "Layer to start from. This is useful for ignoring early Transformer layers, which attend to many tokens."
        }
    )

    weighted_query: bool = field(
        default=True,
        metadata={
            "help": "Whether to use weighted query for SpecPC. If True, the query will be weighted linearly."
        }
    )

    @property
    def sparse_type(self):
        return "specpc"

    def __post_init__(self):
        if self.neighbor_tokens is None:
            self.neighbor_tokens = self.kernel_size


@dataclass
class DraftApproxLLMModelOutput:
    """
    Represents the output of the Draft Approximate LLM model.

    Attributes:
        sequences (torch.Tensor): The tensor containing the sequences generated by the model.
        if self.input_size > self.sequences.size(1):
            raise ValueError(f"input_size ({self.input_size}) exceeds the number of columns in sequences ({self.sequences.size(1)}).")
        return self.sequences[:, :self.input_size]
    """
    sequences: torch.Tensor
    input_size: int

    @property
    def input_ids(self):
        if not (0 <= self.input_size <= self.sequences.size(1)):
            raise ValueError(f"Invalid input_size: {self.input_size}. Must be between 0 and {self.sequences.size(1)}.")
        return self.sequences[:, :self.input_size]

    @property
    def output_ids(self):
        if not (0 <= self.input_size <= self.sequences.size(1)):
            raise ValueError(f"Invalid input_size: {self.input_size}. Must be between 0 and {self.sequences.size(1)}.")
        return self.sequences[:, self.input_size:]


def patch_model(model, draft_model, config):
    from draft_approx_llm.specpc.specpc import specpc_patch_model
    from draft_approx_llm.speckv.speckv import speckv_patch_model

    patch_fn = {
        "speckv": speckv_patch_model,
        "specpc": specpc_patch_model,
    }.get(config.sparse_type, None)

    if patch_fn is None:
        raise ValueError(f"Unsupported sparse type: {config.sparse_type}. Must be one of {list(patch_fn.keys())}.")

    return patch_fn(model, draft_model, config)
